{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaad0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle die Ordnerstruktur\n",
    "import os\n",
    "os.makedirs(\"sql_project/scripts\", exist_ok=True)\n",
    "os.makedirs(\"sql_project/configs\", exist_ok=True)\n",
    "os.makedirs(\"sql_project/outputs\", exist_ok=True)\n",
    "\n",
    "# 2. Wechsel in das Projektverzeichnis\n",
    "%cd sql_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7209191",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "torch\n",
    "transformers\n",
    "peft\n",
    "bitsandbytes\n",
    "trl\n",
    "accelerate\n",
    "datasets\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c83b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "def main(args):\n",
    "    # Hugging Face Hub Login\n",
    "    if args.push_to_hub:\n",
    "        print(\"Initialisiere Hugging Face Hub...\")\n",
    "        try:\n",
    "            # Try to login - if token is provided via environment or file, it will use that\n",
    "            # Otherwise, it will prompt for login\n",
    "            hf_token = os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            if hf_token:\n",
    "                login(token=hf_token)\n",
    "                print(\"‚úì Erfolgreich bei Hugging Face angemeldet!\")\n",
    "            else:\n",
    "                print(\"Bitte melden Sie sich bei Hugging Face an...\")\n",
    "                login()  # This will prompt for token or use existing credentials\n",
    "                print(\"‚úì Erfolgreich bei Hugging Face angemeldet!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Hugging Face Login Fehler: {e}\")\n",
    "            print(\"‚ö†Ô∏è Adapter wird nur lokal gespeichert.\")\n",
    "            args.push_to_hub = False\n",
    "    \n",
    "    # Wandb Setup - Automatically use existing account (option 2)\n",
    "    # Set environment to use existing credentials\n",
    "    os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "    # If wandb is not already logged in, this will use existing credentials from ~/.netrc or environment\n",
    "    try:\n",
    "        import wandb\n",
    "        # Check if wandb is already initialized - use a safer check\n",
    "        try:\n",
    "            current_run = wandb.run\n",
    "        except:\n",
    "            current_run = None\n",
    "        \n",
    "        if current_run is None:\n",
    "            wandb.init(project=\"sql-assistant\", mode=\"online\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization note: {e}\")\n",
    "        # Continue without wandb if there's an issue\n",
    "    # 1. Datensatz laden\n",
    "    print(f\"Lade Datensatz: {args.dataset_name}\")\n",
    "    dataset = load_dataset(args.dataset_name, split=\"train\")\n",
    "    # Nur f√ºr Demo-Zwecke verk√ºrzen, falls gew√ºnscht\n",
    "    if args.max_samples:\n",
    "        dataset = dataset.select(range(args.max_samples))\n",
    "    \n",
    "    # 2. Modell & Tokenizer laden (4-bit QLoRA)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(f\"Lade Modell: {args.model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 3. LoRA Config - f√ºr Qwen2.5 Modelle\n",
    "    peft_config = LoraConfig(\n",
    "        r=16, \n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    \n",
    "    # 4. Model f√ºr k-bit training vorbereiten und PEFT anwenden\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # 5. Training Arguments - Kombiniere TrainingArguments mit SFTConfig\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=args.lr,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=args.epochs,\n",
    "        fp16=False,  # Disable fp16 to avoid BFloat16 gradient scaler issue\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_length=512,\n",
    "        packing=False,\n",
    "        max_grad_norm=1.0  # Gradient clipping\n",
    "    )\n",
    "\n",
    "    # 6. Daten formatieren (Qwen Template)\n",
    "    def format_prompt(sample):\n",
    "        prompt = f\"<|im_start|>system\\nYou are a SQL expert.<|im_end|>\\n<|im_start|>user\\n{sample['context']}\\nQuestion: {sample['question']}<|im_end|>\\n<|im_start|>assistant\\n{sample['answer']}<|im_end|>\"\n",
    "        return {\"text\": prompt}\n",
    "\n",
    "    train_dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "\n",
    "    # 7. Trainer Starten (ohne peft_config, da Modell bereits PEFT-wrapped ist)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "    print(\"Starte Training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Speichere Adapter nach {args.output_dir}...\")\n",
    "    trainer.model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "    # Upload to Hugging Face Hub\n",
    "    if args.push_to_hub:\n",
    "        print(f\"\\nüì§ Lade Adapter auf Hugging Face hoch: {args.hf_repo_id}\")\n",
    "        try:\n",
    "            # Push adapter to Hub\n",
    "            trainer.model.push_to_hub(\n",
    "                args.hf_repo_id,\n",
    "                private=args.private_repo,\n",
    "                token=os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            )\n",
    "            print(f\"‚úì Adapter erfolgreich hochgeladen: https://huggingface.co/{args.hf_repo_id}\")\n",
    "            \n",
    "            # Also push tokenizer\n",
    "            tokenizer.push_to_hub(\n",
    "                args.hf_repo_id,\n",
    "                private=args.private_repo,\n",
    "                token=os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            )\n",
    "            print(\"‚úì Tokenizer erfolgreich hochgeladen!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fehler beim Hochladen auf Hugging Face: {e}\")\n",
    "            print(\"‚ö†Ô∏è Adapter wurde lokal gespeichert.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"b-mc2/sql-create-context\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs/final_model\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=500) # Klein halten f√ºr Test\n",
    "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Upload adapter to Hugging Face Hub\")\n",
    "    parser.add_argument(\"--hf_repo_id\", type=str, default=None, help=\"Hugging Face repo ID (e.g., 'username/model-name')\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None, help=\"Hugging Face token (or set HF_TOKEN env var)\")\n",
    "    parser.add_argument(\"--private_repo\", action=\"store_true\", help=\"Make the Hugging Face repo private\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate Hugging Face arguments\n",
    "    if args.push_to_hub and not args.hf_repo_id:\n",
    "        raise ValueError(\"--hf_repo_id is required when --push_to_hub is set\")\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/evaluate.py\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def normalize_sql(query):\n",
    "    \"\"\"Bereinigt SQL von Leerzeichen und Gro√ü/Kleinschreibung f√ºr fairen Vergleich\"\"\"\n",
    "    if not query: return \"\"\n",
    "    query = query.lower().replace(\";\", \"\").replace(\"\\n\", \" \")\n",
    "    return \" \".join(query.split())\n",
    "\n",
    "def main(args):\n",
    "    # 1. Basis-Modell & Adapter laden\n",
    "    print(f\"Lade Basis-Modell: {args.base_model_name}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(f\"Lade Adapter: {args.adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, args.adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name)\n",
    "\n",
    "    # 2. Test-Daten laden (Die letzten 100 Zeilen des Datasets als Testset nehmen)\n",
    "    dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "    test_dataset = dataset.select(range(len(dataset)-args.num_samples, len(dataset)))\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    print(f\"Starte Evaluation auf {args.num_samples} Beispielen...\")\n",
    "\n",
    "    for sample in tqdm(test_dataset):\n",
    "        # Prompt bauen\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a SQL expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{sample['context']}\\nQuestion: {sample['question']}\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generieren\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "        \n",
    "        # Antwort extrahieren\n",
    "        generated_full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_sql = generated_full.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        # Vergleichen (Normalized Exact Match)\n",
    "        truth_norm = normalize_sql(sample[\"answer\"])\n",
    "        pred_norm = normalize_sql(generated_sql)\n",
    "\n",
    "        if truth_norm == pred_norm:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "\n",
    "    accuracy = (correct_count / total_count) * 100\n",
    "    print(f\"\\n==========================================\")\n",
    "    print(f\"RESULTAT: Exact Match Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"==========================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "    parser.add_argument(\"--adapter_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=50)\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb Setup - Use existing account (option 2) non-interactively\n",
    "import os\n",
    "# Set wandb to use existing account without prompt\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# If you have a wandb API key, you can set it here to avoid prompts:\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c37fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir trainieren das Modell und speichern es im Ordner 'outputs/v1'\n",
    "# Automatisch Option 2 (Use existing W&B account) ausw√§hlen\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def run_with_auto_input(push_to_hub=False, hf_repo_id=None, hf_token=None, private_repo=False):\n",
    "    \"\"\"Run training script and automatically send '2' when wandb prompts\"\"\"\n",
    "    cmd = [sys.executable, \"scripts/train.py\",\n",
    "         \"--model_name\", \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "         \"--output_dir\", \"./outputs/v1\",\n",
    "         \"--epochs\", \"1\",\n",
    "         \"--max_samples\", \"500\"]\n",
    "    \n",
    "    # Add Hugging Face upload arguments if specified\n",
    "    if push_to_hub:\n",
    "        cmd.extend([\"--push_to_hub\"])\n",
    "        if hf_repo_id:\n",
    "            cmd.extend([\"--hf_repo_id\", hf_repo_id])\n",
    "        if hf_token:\n",
    "            cmd.extend([\"--hf_token\", hf_token])\n",
    "        if private_repo:\n",
    "            cmd.extend([\"--private_repo\"])\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    buffer = \"\"\n",
    "    sent_input = False\n",
    "    \n",
    "    while True:\n",
    "        # Read available output\n",
    "        output = process.stdout.read(1)\n",
    "        if not output:\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "            \n",
    "        buffer += output\n",
    "        print(output, end='', flush=True)\n",
    "        \n",
    "        # Check for wandb prompt and send \"2\" once\n",
    "        if \"Enter your choice:\" in buffer and not sent_input:\n",
    "            time.sleep(0.2)\n",
    "            process.stdin.write(\"2\\n\")\n",
    "            process.stdin.flush()\n",
    "            sent_input = True\n",
    "            buffer = \"\"\n",
    "        \n",
    "        # Keep buffer size manageable\n",
    "        if len(buffer) > 500:\n",
    "            buffer = buffer[-200:]\n",
    "    \n",
    "    return process.wait()\n",
    "\n",
    "# Training OHNE Hugging Face Upload (Standard)\n",
    "run_with_auto_input()\n",
    "\n",
    "# Training MIT Hugging Face Upload (auskommentiert - bitte aktivieren und anpassen)\n",
    "# run_with_auto_input(\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_id=\"your-username/your-model-name\",  # Z.B. \"manuelaschrittwieser99/qwen2.5-1.5b-sql-adapter\"\n",
    "#     hf_token=None,  # Optional: Token direkt √ºbergeben, oder setzen Sie HF_TOKEN als Umgebungsvariable\n",
    "#     private_repo=False  # True = privates Repo, False = √∂ffentliches Repo\n",
    "# )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload eines bereits trainierten Adapters zu Hugging Face\n",
    "import torch\n",
    "from huggingface_hub import login, HfApi\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "def upload_adapter_to_hub(\n",
    "    adapter_path=\"./outputs/v1\",\n",
    "    hf_repo_id=None,\n",
    "    base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    hf_token=None,\n",
    "    private_repo=False\n",
    "):\n",
    "    \"\"\"\n",
    "    L√§dt einen bereits trainierten Adapter und l√§dt ihn auf Hugging Face hoch.\n",
    "    \n",
    "    Args:\n",
    "        adapter_path: Pfad zum lokal gespeicherten Adapter\n",
    "        hf_repo_id: Hugging Face Repo ID (z.B. \"username/model-name\")\n",
    "        base_model_name: Name des Basis-Modells\n",
    "        hf_token: Hugging Face Token (optional, kann auch als HF_TOKEN env var gesetzt werden)\n",
    "        private_repo: Ob das Repo privat sein soll\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hf_repo_id:\n",
    "        raise ValueError(\"hf_repo_id ist erforderlich (z.B. 'username/model-name')\")\n",
    "    \n",
    "    # Login zu Hugging Face\n",
    "    print(\"Initialisiere Hugging Face Hub...\")\n",
    "    token = hf_token or os.environ.get(\"HF_TOKEN\")\n",
    "    if token:\n",
    "        login(token=token)\n",
    "        print(\"‚úì Erfolgreich bei Hugging Face angemeldet!\")\n",
    "    else:\n",
    "        print(\"Bitte melden Sie sich bei Hugging Face an...\")\n",
    "        login()\n",
    "        print(\"‚úì Erfolgreich bei Hugging Face angemeldet!\")\n",
    "    \n",
    "    # Lade Basis-Modell (nur zum Push ben√∂tigt, wird nicht geladen)\n",
    "    print(f\"\\nüì§ Lade Adapter von: {adapter_path}\")\n",
    "    \n",
    "    # F√ºr PEFT-Modelle m√ºssen wir das Basis-Modell laden, um den Adapter zu pushen\n",
    "    # Aber wir k√∂nnen es im 4-bit Modus laden, um Speicher zu sparen\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        print(f\"Lade Basis-Modell: {base_model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, \n",
    "            quantization_config=bnb_config, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Konnte Modell nicht im 4-bit Modus laden: {e}\")\n",
    "        print(\"Versuche ohne Quantisierung...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    \n",
    "    # Lade Adapter\n",
    "    print(f\"Lade Adapter von: {adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Lade Tokenizer\n",
    "    print(\"Lade Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Upload Adapter\n",
    "    print(f\"\\nüì§ Lade Adapter auf Hugging Face hoch: {hf_repo_id}\")\n",
    "    try:\n",
    "        model.push_to_hub(\n",
    "            hf_repo_id,\n",
    "            private=private_repo,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"‚úì Adapter erfolgreich hochgeladen: https://huggingface.co/{hf_repo_id}\")\n",
    "        \n",
    "        # Upload Tokenizer\n",
    "        tokenizer.push_to_hub(\n",
    "            hf_repo_id,\n",
    "            private=private_repo,\n",
    "            token=token\n",
    "        )\n",
    "        print(\"‚úì Tokenizer erfolgreich hochgeladen!\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Fertig! Adapter ist verf√ºgbar unter: https://huggingface.co/{hf_repo_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler beim Hochladen: {e}\")\n",
    "        raise\n",
    "\n",
    "# BEISPIEL-VERWENDUNG:\n",
    "# Bitte passen Sie die Parameter an:\n",
    "\n",
    "# upload_adapter_to_hub(\n",
    "#     adapter_path=\"./outputs/v1\",  # Pfad zu Ihrem trainierten Adapter\n",
    "#     hf_repo_id=\"your-username/your-model-name\",  # Ihr Hugging Face Repo\n",
    "#     base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # Basis-Modell\n",
    "#     hf_token=None,  # Optional: Token hier angeben, oder HF_TOKEN env var setzen\n",
    "#     private_repo=False  # True f√ºr privates Repo\n",
    "# )\n",
    "\n",
    "print(\"‚úì Upload-Funktion bereit!\")\n",
    "print(\"\\nZum Hochladen, f√ºhren Sie folgenden Code aus:\")\n",
    "print(\"upload_adapter_to_hub(\")\n",
    "print('    adapter_path=\"./outputs/v1\",')\n",
    "print('    hf_repo_id=\"your-username/your-model-name\",')\n",
    "print('    private_repo=False')\n",
    "print(\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/deploy.py\n",
    "import argparse\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "def main(args):\n",
    "    api = HfApi()\n",
    "    \n",
    "    # 1. Repo-Namen bauen\n",
    "    full_repo_id = f\"{args.username}/{args.repo_name}\"\n",
    "    print(f\"Ziel-Repository: {full_repo_id}\")\n",
    "\n",
    "    # 2. Repository erstellen (falls es noch nicht existiert)\n",
    "    try:\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "        print(\"Repository gefunden oder erstellt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Hinweis beim Repo-Erstellen: {e}\")\n",
    "\n",
    "    # 3. Dateien hochladen\n",
    "    print(f\"Lade Ordner '{args.model_dir}' hoch... Bitte warten.\")\n",
    "    \n",
    "    api.upload_folder(\n",
    "        folder_path=args.model_dir,\n",
    "        repo_id=full_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=f\"Upload model from production script: {args.repo_name}\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Upload erfolgreich!\")\n",
    "    print(f\"Dein Modell ist hier: https://huggingface.co/{full_repo_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--username\", type=str, required=True, help=\"Hugging Face Nutzername\")\n",
    "    parser.add_argument(\"--repo_name\", type=str, required=True, help=\"Name f√ºr das neue Modell auf HF\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=\"./outputs/final_model\", help=\"Lokaler Pfad zum Modell\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_agent.py --adapter \"manuelaschrittwieser/Qwen2.5-SQL-Assistant-Prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/deploy.py \\\n",
    "    --username \"manuelaschrittwieser\" \\\n",
    "    --repo_name \"Qwen2.5-SQL-Assistant-Prod\" \\\n",
    "    --model_dir \"./outputs/v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch transformers peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/setup_db.py\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_dummy_db(db_path=\"data/dummy_database.db\"):\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Tabelle erstellen\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER,\n",
    "        hire_date DATE\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Daten einf√ºgen\n",
    "    employees = [\n",
    "        (1, 'Alice Smith', 'Sales', 55000, '2021-01-15'),\n",
    "        (2, 'Bob Jones', 'Engineering', 85000, '2020-03-10'),\n",
    "        (3, 'Charlie Brown', 'Sales', 48000, '2022-06-23'),\n",
    "        (4, 'Diana Prince', 'Engineering', 92000, '2019-11-05'),\n",
    "        (5, 'Evan Wright', 'HR', 45000, '2021-09-30')\n",
    "    ]\n",
    "    \n",
    "    cursor.executemany('INSERT OR IGNORE INTO employees VALUES (?,?,?,?,?)', employees)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"‚úÖ Datenbank erstellt: {db_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dummy_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/run_agent.py\n",
    "import sqlite3\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "class SQLAgent:\n",
    "    def __init__(self, base_model_id, adapter_id, db_path):\n",
    "        self.db_path = db_path\n",
    "        print(\"ü§ñ Lade das Gehirn des Agenten...\")\n",
    "        \n",
    "        # Modell laden\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        self.model = PeftModel.from_pretrained(base_model, adapter_id)\n",
    "        \n",
    "    def generate_sql(self, question, schema_context):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a SQL expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{schema_context}\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=100)\n",
    "            \n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extrahiere alles nach 'assistant'\n",
    "        if \"assistant\" in full_text:\n",
    "            return full_text.split(\"assistant\")[-1].strip()\n",
    "        return full_text\n",
    "\n",
    "    def execute_sql(self, query):\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            conn.close()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return f\"Fehler bei SQL-Ausf√ºhrung: {e}\"\n",
    "\n",
    "    def run(self):\n",
    "        schema = \"CREATE TABLE employees (id INTEGER, name TEXT, department TEXT, salary INTEGER, hire_date DATE)\"\n",
    "        print(\"\\n‚úÖ Agent bereit! Tippe 'exit' zum Beenden.\")\n",
    "        \n",
    "        while True:\n",
    "            # Hier wartet Colab auf deine Eingabe\n",
    "            user_input = input(\"\\nDeine Frage an die Datenbank: \")\n",
    "            \n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"üëã Bis bald!\")\n",
    "                break\n",
    "                \n",
    "            # 1. Denken (SQL generieren)\n",
    "            sql = self.generate_sql(user_input, schema)\n",
    "            print(f\"üß† Gedanke (SQL): {sql}\")\n",
    "            \n",
    "            # 2. Handeln (SQL ausf√ºhren)\n",
    "            data = self.execute_sql(sql)\n",
    "            \n",
    "            # 3. Antworten\n",
    "            print(f\"üìä Ergebnis aus DB: {data}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--adapter\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    agent = SQLAgent(\n",
    "        base_model_id=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        adapter_id=args.adapter,\n",
    "        db_path=\"data/dummy_database.db\"\n",
    "    )\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/setup_db.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir evaluieren das Modell aus dem Ordner 'outputs/v1'\n",
    "!python scripts/evaluate.py \\\n",
    "    --adapter_path \"./outputs/v1\" \\\n",
    "    --num_samples 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
